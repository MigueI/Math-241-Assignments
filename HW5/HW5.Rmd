---
title: "MATH 241: Homework 05"
author: "Miguel Conner"
date: "Due Wednesday 2015/4/29 1:00pm on Moodle"
output: html_document
---

Please indicate

* Approximately how much time you spent on this homework: 4
* What, if anything, gave you an exceeding amount of trouble?  By exceeding I mean above and beyond run-of-the-mill difficulties when learning new things.   

```{r, echo=FALSE, warning=FALSE, message=FALSE}
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(wordcloud))
suppressPackageStartupMessages(library(tm))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(htmlwidgets))
suppressPackageStartupMessages(library(DT))
#suppressPackageStartupMessages(library(sentiment))
suppressPackageStartupMessages(library(twitteR))

```



## Question 1:

Let's analyze some of the text from the following sources:

1. Reed College subreddit,
2. Portland subreddit,
3. A scraping of all tweets using a word, hashtag, Twitter ID of your choice for a given date range defined by `start.date` and `end.date` (geo-location is optional),
4. Sir Arthur Conan Doyle's novel called The Lost World [Project Gutenberg](http://www.gutenberg.org/).

I'll present the top `n` "interesting" words used in each of the four data sets, in a word cloud and a data table.    
    
Many thanks to Mitchell Linegar for gathering the data.  The python code used to scrape the data can be found [here](https://github.com/mlinegar/RedditScraping).  

```{r}
# Define parameters. 
n <- 30                     # Number of words in word cloud.
nt <- 1000                 # Number of tweets searched.
start.date <- "2015-01-01"  # Start date of Twitter search.
end.date <- "2015-04-30"    # End date of Twitter search.
topic <- "data science"     # 'topic' defines what is searched in Twitter. 
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Import data.
reed <- read.csv("reedcollege.comments.csv", stringsAsFactors = FALSE, header=FALSE) %>%
  tbl_df() 
portland <- read.csv("portland.comments.csv", stringsAsFactors = FALSE, header=FALSE) %>%
  tbl_df()
the.lost.world <- readLines("The_Lost_World.txt", encoding="UTF-8")
```

### Reed College subreddit

```{r, echo=FALSE, message=FALSE}
# Start by cleaning text. 
reed$V1 %<>% tolower() %>%
  removeNumbers() %>%
  removePunctuation() %>%
  removeWords(stopwords("english")) %>%
  stemDocument() %>%
  stripWhitespace()

# Make word cloud.
invisible(VectorSource(reed$V1) %>% Corpus())
wordcloud(reed$V1, scale=c(5,0.5), max.words=n, random.order=FALSE,
          rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Reds"))
title(paste(paste("Top",n),"words in\nr/Reed"))
```

We see that a lot of the words indicate personal opinions, presumably becuase the subreddit is filled with questions from prospective students which are then answered by current students. 

```{r, echo=FALSE, warnings= FALSE}
# Make list of top 'n' words.
top.reed <- str_split(reed$V1, pattern= " ") %>%
  unlist() %>%
  table() %>%
  sort(decreasing=TRUE) %>%
  .[1:n] %>%
  data.frame()

# Name columns
 top.reed <- cbind(rownames(top.reed), top.reed)
 rownames(top.reed) <- NULL
 colnames(top.reed) <- c("Word","Counts")

# Make data table.
 datatable(top.reed)
```

### Portland subreddit

```{r, echo=FALSE, warning=FALSE}
portland$V1 %<>% tolower() %>%
  removeNumbers() %>%
  removePunctuation() %>%
  removeWords(stopwords("english")) %>%
  stemDocument() %>%
  stripWhitespace()

# Make word cloud.
invisible(VectorSource(portland$V1) %>% Corpus())
wordcloud(portland$V1, scale=c(5,0.5), max.words=n, random.order=FALSE, 
          rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "YlGn"))
title(paste(paste("Top",n),"words in\nr/Portland"))
```

It's difficult to find a really clear topic or pattern, but it appears that many of the words have positive associatios (e.g. "like","good","well", "think", "make").

```{r, echo=FALSE, warning=FALSE}
# Make list of top 'n' words.
top.pdx <- str_split(portland$V1, pattern= " ") %>%
  unlist() %>%
  table() %>%
  sort(decreasing=TRUE) %>%
  .[1:n] %>%
  data.frame()

# Name columns
top.pdx <- cbind(rownames(top.pdx), top.pdx)
rownames(top.pdx) <- NULL
colnames(top.pdx) <- c("Word","Counts")

# Make data table.
datatable(top.pdx)
``` 

### Twitter

```{r, echo=FALSE, warnings=FALSE}
# Get past tweets about 'topic' from twitter from 'start.date' until 'end.date'.
base_tweets <- searchTwitter(topic, lang="en",n=nt, since=start.date, until=end.date) %>%
  sapply(function(x) x$getText()) %>%
  data.frame()


# Clean it up.  
tweets <- VectorSource(base_tweets$.) %>%
  Corpus() %>%  
  tm_map( function(x) iconv(x, to='UTF-8-MAC', sub='byte')) %>%
  tolower() %>%
  removeNumbers() %>%
  removePunctuation() %>%
  removeWords(stopwords("english")) %>%
  stemDocument() %>%
  stripWhitespace()

# Make wordcloud.
# For some reason on twitter, there seem to be some words that appear VERY frequently, 
# and everything else isn't as common. For this reason, it's difficult to see the 
# uncommon words with sequential color scheme that fades away. It's better to use
# a qualitative color scheme, and base word frequency on the size of the word. 
wordcloud(tweets, max.words=100,scale=c(5,0.5), random.order=FALSE, 
          rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))
title(paste(paste(paste(paste(paste("Tweets about", topic),"from\n "),start.date), "to"),end.date))
```

We look at the first 1000 tweets about `data science` since the start of the year. The big ones are "data", "science", "bigdata", and "datascience", which are not huge surprises since those are the terms we used. It turns out there's another interesting grouping of words: "mars", "nasa", "curiosity", "rover", "marsrover", etc.. I actually spent last summer at the University of Arkansas doing physics research and analyzing data from these rovers; I was ahead of the curve! Also, there are a ouple of buissiness/industry terms and academic terms. Finally, "orgasm" reveals the heightened sexual appeal that accompanies data science.

```{r, echo=FALSE, warning=FALSE}
# Make list of top 'n' words.
top.tweets <- str_split(tweets, pattern= " ") %>%
  unlist() %>%
  table() %>%
  sort(decreasing=TRUE) %>%
  .[1:50] %>%
  data.frame()

# Name columns
top.tweets <- cbind(rownames(top.tweets), top.tweets)
rownames(top.tweets) <- NULL
colnames(top.tweets) <- c("Word","Counts")

# Make data table.
datatable(top.tweets)
```

### The Lost World -- Sir Arthur Conan Doyle

Sir Arthur Conan Doyle was most famous for creating Sherlock Holmes, but he also wrote a couple of other books, one of which I happened upon one summer in highschool. In \underline{The Lost World}, Doyle tells recounts the story of a group of English adventureres who travel to South America and stumble upon an isolated platuea with... DINOSAURS! 

```{r, echo=FALSE}
the.lost.world %<>% tolower() %>%
  removeNumbers() %>%
  removePunctuation() %>%
  removeWords(stopwords("english")) %>%
  stemDocument()
  
book <- the.lost.world %>%
  paste(collapse=" ") %>%
  stripWhitespace() %>%
  str_split(pattern=" ") %>%
  table() %>%
  sort(decreasing=TRUE) %>%
  .[1:n] %>%
  data.frame()

# Make wordcloud.
invisible(VectorSource(the.lost.world) %>% Corpus())
wordcloud(the.lost.world, scale=c(5,0.5), max.words=n, random.order=FALSE,
          rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "BuPu"))
title(paste(paste("Top",n),"words in\n`The Lost World`"))
```

With the word cloud we get a sense for some of the characters in the novel (i.e. professor Challenger, professor Summerlee, and lord John Roxton).

```{r, echo=FALSE}
# Name columns.
book <- cbind(rownames(book), book)
rownames(book) <- NULL
colnames(book) <- c("Word","Counts")

# Data table. 
datatable(book)
```
