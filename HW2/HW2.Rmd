---
title: "MATH 214: Homework 02"
author: "Miguel Conner"
output: html_document
---

```{r, echo=FALSE}
suppressPackageStartupMessages(library(foreign))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(ROCR))
suppressPackageStartupMessages(require(gridExtra))
```

```{r, echo=FALSE}
url <- "http://www.stat.columbia.edu/~gelman/arm/examples/pollution/pollution.dta"
pollution <- read.dta(url) %>% tbl_df()
```

## Question 1:

*Logarithmic transformations: the folder pollution contains mortality rates and various environmental factors from 60 U.S. metropolitan areas (see McDonald and Schwing, 1973). For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.*

### a) Linear Regression Doesn't Work
*Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.*

```{r}
p <- ggplot(pollution, aes(nox, mort)) + 
  geom_point() + 
  xlab("Level of Nitric Oxides") + 
  ylab("Mortality Rate")

model2 <- lm(mort ~ nox, data=pollution)
b <- coefficients(model2)
p + geom_abline(intercept=b[1], slope=b[2], col="yellowgreen", size=1) +
  ggtitle("Linear Regression")
```

Just to emphasize how bad that looks, let's make a residual plot from the regression:

```{r}
rs <- resid(model2)
p <- ggplot(data.frame(x=pollution$nox,y=rs), aes(x=pollution$nox,y=rs)) + 
  geom_point() +
  ggtitle("Linear Residual Plot") +
  xlab("Level of Nitric Oxides") + 
  ylab("Residual")
p + geom_hline(yintercept=0, col="yellowgreen") 
```

Ouch, that doesn't look great. 

### b) Log Transformation
*Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.*

Try taking the log of the nitric oxide levels, then fit that with a linear model.

```{r}
p <- ggplot(pollution, aes(log(nox), mort)) + 
  geom_point() + 
  xlab("log(Level of Nitric Oxides)") + 
  ylab("Mortality Rate")

model2 <- lm(mort ~ log(nox), data=pollution)
b <- coefficients(model2)
p + geom_abline(intercept=b[1], slope=b[2], col="yellowgreen", size=1) +
  ggtitle("Log Transformation and Linear Regression")
```

...and the residuals now look like:

```{r}
rs<- resid(model2)
p <- ggplot(data.frame(x=log(pollution$nox),y=rs), aes(x=log(pollution$nox),y=rs)) +
  geom_point() + 
  ggtitle("Log Residual Plot") +
  xlab("log(Level of Nitric Oxides)") + 
  ylab("Residual")
p + geom_hline(yintercept=0, col="yellowgreen") 
```

This looks a lot better since the data points are centered around the line at y = 0.

### c) Slope
*Interpret the slope coefficient from the model you chose in (b).*

```{r}
b[2]
```

For every increase in log(NOX Levels), we get an increase in +15.3 of the mortality rate.  

### d) Make a Model
*Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when helpful. Plot the fitted regression model and interpret the coefficients.*

We'll make a plot of all three to get a sense of how they interact. 

```{r}
p <- ggplot(pollution, aes(x=log(nox), y=mort, color=so2)) + 
  geom_point(size=log(pollution$hc)) +
  xlab("log(Nitric Oxides Levels)") + 
  ylab("Mortality Rate")
p <- p + scale_colour_gradient("SO2 Levels", high = "orange", low = "blue") 
p + ggtitle("log(NOX), SO2, and log(HC) Levels vs. Mortality")
``` 

Note that the size of the point is proportional to the log of the amount of hydrocarbons.

What does this graph tell us? As we have already seen, an increase in nitric oxides corresponds to an increase in mortality. This graph also suggests that SO2 levels are correlated with increased mortality (yellower dots seem to appear in the top right corner of the graph). The amount of hydrocarbons, judging by the size of the data points in the lower right corner of the graph, don't seem to affect mortality. Since nitric oxides and sulfur dioxide levels seem to be the most important, lets use a multivariable model to analyze the effect of these two factors.

```{r}
# Create new category for higher than average sulfur levels.
pollution %<>% mutate(above.avg.SO2 = ifelse(so2 > mean(so2),1,0))

model4 <- lm(mort ~ as.factor(above.avg.SO2) * log(nox), data=pollution)
summary(model4)
b <- coefficients(model4)
b

p <- ggplot(pollution, aes(x=log(nox), y=mort, color=as.factor(above.avg.SO2))) + 
  geom_point(size = 3) + 
  xlab("log(Nitric Oxides Levels)") + 
  ylab("Mortality Rate")
p + geom_abline(intercept=b[1], slope=b[3], col="#F8766D", size=1) +
  geom_abline(intercept=b[1]+b[2], slope=b[3]+b[4], col="#00BFC4", size=1)
```

In this case, our model spits out four coefficients. Since we're fitting two lines, we need two different slope intercepts and two different slopes. The first coefficient is the intercept of the bottom line (lower than average sulfur dioxide levels). The second coefficient is the additional distance, from the first intercept to the intercept of the top line (higher than average sulfur dioxide levels). Finally, the third coefficient gives the slope of the first line, and the sum of the third and fourth coefficients give the slope of the second line. Our interaction model is fitting a coefficient for the log of the nitric oxide levels, a coefficient for the above/below average sulfur dioxide levels, and then a third coefficient for both of these terms.  

### e) Test the Model
*Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in (d), so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)*

We're going to divide our data into two: a and b. We'll test our model on a, and see how well it predicts our b.

```{r}
# Split into halfs. 
pollutiona <- slice(pollution, 1:30) 
pollutionb <- slice(pollution, 31:60) 

# Apply model to first half.
model4a <- lm(mort ~ as.factor(above.avg.SO2) * log(nox), data=pollutiona)
ba <- coefficients(model4a)
```

How well does the first half predict the second half? 

```{r}
# Make predictions.
pollutionb$predictions <- predict(model4a, pollutionb)

# Plot predictions.
ggplot(pollutionb, aes(mort, predictions)) + geom_point(size = 3) + 
  geom_abline(intercept = 0, size = 1) + 
  xlab("Actual Deaths") + 
  ylab("Predicted Deaths") + 
  coord_cartesian(xlim = c(775, 1125), ylim = c(775, 1125)) + 
  ggtitle("Using log(NOX) and Above Avg. SO2 Levels as Predictors")
```

An perfect prediction puts all points on the line x = y. Here we see that all of our data points surround this line, but we also have more than a few outliers. 

We can also try using all three metrics of air pollution (NOX, SO2, and HC), though this is a little harder to see graphically. Our interaction model starts with all three variables. 

```{r}
# Make new model.
allmodela <- lm(mort ~ so2 * log(nox) * log(hc), data=pollutiona)
allba <- coefficients(allmodela)
allba
```

We get a coefficient for each predictor on its own, then another for each combination interaction pair, a predictor for an interaction between the three, and then the slope intercept. Note that the largest effects come from log(nox), log(hc), and the interaction between log(nox) and log(hc).

```{r}
# Use model to make predictions, then plot them.
pollutionb$allpredictions <- predict(allmodela, pollutionb)
ggplot(pollutionb, aes(mort, allpredictions)) + 
  geom_point(size = 3) + 
  geom_abline(intercept = 0, size = 1) + 
  xlab("Actual Deaths") + 
  ylab("Predicted Deaths") + 
  coord_cartesian(xlim = c(775, 1125), ylim = c(775, 1125)) + 
  ggtitle("Using log(NOX), SO2, and log(HC) Levels as Predictors ")
```

This seems to fit a lot better (we've got a couple of points on the line), but we still have a few outliers. 

## Question 2:

*Using the OkCupid data, fit what you think is a good predictive model for gender and interpret all results.*

*Think about what happens when your prediction mechanism is overly optimized for your particular dataset. What will happen when it tries to predict other datasets?*

We need to be careful about our prediction method: too specific and we "overfit" the data, but too general and we don't gain any meaningful information. We're looking for the sweetspot in between. We can also avoid this problem by looking for distinct differences in large populations, since they will likely to hold for new data as well. To this end, our model will require only three inputs: 

* whether or not they work in tech,

* height,

* and 'curvy' or 'atheltic' body types.

I will give justification for including all three of these parameters, then make a model, train it, and finally, test it.

```{r, echo= FALSE}
#Import and clean 'profiles' data.
profiles <- read.csv("profiles.csv", header=TRUE) %>% tbl_df()

# Split off the essays into a separate data.frame
essays <- select(profiles, contains("essay"))
profiles <- select(profiles, -contains("essay"))

# Define a binary outcome variable
# y_i = 1 if female
# y_i = 0 if male
profiles <- mutate(profiles, is.female = ifelse(sex=="f", 1, 0))

# Last Online
profiles$last_online <- str_sub(profiles$last_online, 1, 10) %>% as.Date()
```

### Heights 

A quick histogram shows that the taller person is more likely to be a guy. 

```{r}
ggplot(profiles, aes(height, fill = sex)) + 
  xlab("Heights") +
  ylab("Counts") +
  geom_histogram(binwidth = 1)
```

### Works in Tech or with Computers

Diversity in typically male dominated fields like the tech industry or 'hard sciences' (physics and engineering) has been a difficult issue for our society. Do some of these stereotypes hold for our OKCupid  profiles?

```{r}
ggplot(profiles, aes(job, fill = sex)) + 
  geom_bar(position = "dodge") + 
  xlab("Job") +
  ylab("Counts") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) 
```

It appears that the 'computer / hardware / software' and 'science / tech / engineering' fields are, as per the stereotype, heavily male dominated. While we may recognize that something needs to be done about this issue, we will unashamedly use this in our model since it includes a large and polarized population. We'll group both of these categories into a new variable 'in.tech'.

```{r}
profiles %<>% 
  mutate("in.tech" = ifelse(profiles$job == "computer / hardware / software" | 
                              profiles$job == "science / tech / engineering", 1, 0))
```

### Body Type

Looking at the graph below, we see that 'athletic' and 'curvy' body types could act as good predictors for males and females, respectively. 

```{r}
ggplot(profiles, aes(body_type, fill = sex)) + 
  geom_bar(position = "dodge") + 
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  xlab("Body Type") +
  ylab("Count") 
```

Since these are the two most polarizing answers, we will create new variables for both. 

```{r}
profiles %<>% mutate("is.athletic" = ifelse(profiles$body_type=="athletic", 1, 0))
profiles %<>% mutate("is.curvy" = ifelse(profiles$body_type=="curvy", 1, 0))
```

### Testing Model

```{r}
# Split into halfs. 
profilesa <- slice(profiles, 1:(nrow(profiles)/2)) 
profilesb <- slice(profiles, (nrow(profiles)/2 + 1):nrow(profiles)) 

# Train with first half. 
testmodel <- glm(is.female ~ height + factor(in.tech) + 
        factor(is.athletic) + factor(is.curvy), family = binomial, data = profilesa)

# Make predictions and convert to usable value with inverse logit function.
profilesb$predictions <- predict(testmodel, profilesb)
profilesb$predictions <- 1/(1+exp(-profilesb$predictions))

aggregate(predictions~is.female, data=profilesb, FUN=function(predictions) mean=mean(predictions))

ggplot(profilesb, aes(jitter(is.female), predictions)) + 
  geom_point(size = 2) + 
  geom_abline(intercept = 0, size = 1) + 
  xlab("Is Actually Female") + 
  ylab("Prediction") + 
  ggtitle("Predicting A Female Profile")
```

The results look pretty good... Again, we want the majority of points to lie on (0,0) or (1,1) because it means our model predicts female or male correctly.

### The Actual Model

```{r}
model <- glm(is.female ~ height + factor(in.tech) + factor(is.athletic) +
               factor(is.curvy), family = binomial, data = profiles)
summary(model)

b <- coefficients(model)
b
```

Negative coefficients are nudging our model towards males (is.female = 0), where as positive values are pushing our predictions towards females (is.female = 1). We note that being curvy is the largest coefficient because if someone put that on their profile, we know it's a woman. Our second strongest predictor is 'in.tech' followed by 'is.athletic' and finally 'height'. However, an important note about height is we can use it for every profile, where as the others are hit or miss (either they have the term or they don't). Thus, our model makes really good guesses for certain keywords, but the backbone of the model involves guessing based on height.

Can we visualize whats going on? Let's break it down, and look at only a couple of the coefficients: height and in.tech. We see that if the person is in.tech (blue), the logistic curve bends downwards earlier, since it is more likely to be a guy.

```{r, echo=FALSE}
regression.line <- function(x, y, b){
  linear.equation <- b[1] + b[2]*x + b[3]*y
  1/(1+exp(-linear.equation))
}

p <- ggplot(data=profiles, aes(x=jitter(height), y=jitter(is.female))) +
  geom_point(aes(col=factor(in.tech))) + 
    scale_colour_discrete("in.tech") +
  xlab("Height") + 
  ylab("is female")
p + stat_function(fun = regression.line, args=list(b = b, 0), color="#f75f55", size=2) +
  stat_function(fun = regression.line, args=list(b = b, 1), color="#00a6ab", size=2)
```

We should also compare height and is.curvy, where we really see the size of the coefficient makes a big difference. If our profile is curvy, then we expect a female at most heights. 

```{r, echo=FALSE}
regression.line <- function(x, y, b){
  linear.equation <- b[1] + b[2]*x + b[5]*y
  1/(1+exp(-linear.equation))
}

p <- ggplot(data=profiles, aes(x=jitter(height), y=jitter(is.female))) +
  geom_point(aes(col=factor(is.curvy))) + 
  scale_colour_discrete("is.curvy") +
  xlab("Height") + 
  ylab("is female")
p + stat_function(fun = regression.line, args=list(b = b, 0), color="#f75f55", size=2) +
  stat_function(fun = regression.line, args=list(b = b, 1), color="#00a6ab", size=2)
```
